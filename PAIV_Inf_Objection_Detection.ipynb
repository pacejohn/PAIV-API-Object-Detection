{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Object Detection using Power AI Vision API calls</h1>\n",
    "\n",
    "This notebook allows you to make API calls to a server running Power AI Vision to perform object detection on videos.  \n",
    "\n",
    "Special thanks to Mark Sturdevant from IBM for code.  Check out his GitHub with the code at https://github.com/IBM/powerai-counting-cars/blob/master/README.md."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set this URL to the API endpoint of your deployed model.\n",
    "POWER_AI_VISION_API_URL = \"https://server_IP_address/powerai-vision/api/dlapis/API_Key_goes_here\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output dir to hold/cache the original frames\n",
    "FRAMES_DIR = \"frames\"  \n",
    "# Output dir to hold the annotated frames\n",
    "OUTPUT_DIR = \"output\"  \n",
    "# Classify every n frames (use tracking in between)\n",
    "SAMPLING = 1  \n",
    "# Confidence threshold to filter iffy objects\n",
    "CONFIDENCE = 0.90  \n",
    "# If running the notebook on Windows, the \\ has to be replaced with / in file names and paths.\n",
    "# Use Y if running on Windows or N if running on a flavor of Linux\n",
    "WINDOWS = \"Y\" \n",
    "\n",
    "# OpenCV colors are (B, G, R) tuples -- RGB in reverse\n",
    "WHITE = (255, 255, 255)\n",
    "YELLOW = (66, 244, 238)\n",
    "GREEN = (80, 220, 60)\n",
    "LIGHT_CYAN = (255, 255, 224)\n",
    "DARK_BLUE = (139, 0, 0)\n",
    "GRAY = (128, 128, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "import math\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import cv2\n",
    "from IPython.display import clear_output, Image, display\n",
    "import requests\n",
    "from requests.packages.urllib3.exceptions import InsecureRequestWarning\n",
    "requests.packages.urllib3.disable_warnings(InsecureRequestWarning)\n",
    "\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of input video\n",
    "input_video = \"input_video_name.mp4\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Create the directories</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(FRAMES_DIR):\n",
    "    os.mkdir(FRAMES_DIR)\n",
    "if not os.path.isdir(OUTPUT_DIR):\n",
    "    os.mkdir(OUTPUT_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Parse and explode the video file into JPEGs</h1>\n",
    "\n",
    "Each frame is saved as an individual JPEG file for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the input video\n",
    "if os.path.isfile(input_video):\n",
    "    video_capture = cv2.VideoCapture(input_video)\n",
    "else:\n",
    "    raise Exception(\"File %s doesn't exist!\" % input_video)\n",
    "\n",
    "# Get the total number of frames in the video\n",
    "total_frames = int(video_capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "print(\"Frame count estimate is %d\" % total_frames)\n",
    "\n",
    "# Read each frame and write it to a jpg file\n",
    "num = 0\n",
    "while video_capture.get(cv2.CAP_PROP_POS_FRAMES) < video_capture.get(cv2.CAP_PROP_FRAME_COUNT):\n",
    "    success, image = video_capture.read()\n",
    "    if success:\n",
    "        num = int(video_capture.get(cv2.CAP_PROP_POS_FRAMES))\n",
    "        print(\"Writing frame {num} of {total_frames}\".format(\n",
    "            num=num, total_frames=total_frames), end=\"\\r\")\n",
    "        cv2.imwrite('{frames_dir}/frame_{num:05d}.jpg'.format(\n",
    "            frames_dir=FRAMES_DIR, num=num), image)\n",
    "    else:\n",
    "        # TODO: If this happens, we need to add retry code\n",
    "        raise Exception('Error writing frame_{num:05d}.jpg'.format(\n",
    "            num=int(video_capture.get(cv2.CAP_PROP_POS_FRAMES))))\n",
    "\n",
    "print(\"\\nWrote {num} frames\".format(num=num))\n",
    "\n",
    "FRAME_FPS = int(video_capture.get(cv2.CAP_PROP_FPS))\n",
    "FRAME_WIDTH = int(video_capture.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "FRAME_HEIGHT = int(video_capture.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "print(\"Frame Dimensions: %sx%s\" % (FRAME_WIDTH, FRAME_HEIGHT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>PowerAI Vision inference wrapper</h1>\n",
    "\n",
    "Define a helper/wrapper to call PowerAI Vision and return the inference result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the file and make the API call back to the PAIV server\n",
    "s = requests.Session()\n",
    "\n",
    "def detect_objects(filename):\n",
    "\n",
    "    with open(filename, 'rb') as f:\n",
    "        # WARNING! verify=False is here to allow an untrusted cert!\n",
    "        r = s.post(POWER_AI_VISION_API_URL,\n",
    "                   files={'files': (filename, f)},\n",
    "                   verify=False)\n",
    "\n",
    "    return r.status_code, json.loads(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Test the API on a single frame</h1>\n",
    "\n",
    "Let's look at the result of a single inference operation from the PowerAI Vision Object Detection API. We see a standard HTTP return code, and a JSON response which includes the image URL, and tuples that indicate the confidence and bounding-box coordinates of the objects that we classified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a sanity check to make sure everything is working fine\n",
    "# rc is the response code from the API call\n",
    "rc, jsonresp = detect_objects('frames/frame_00300.jpg')\n",
    "\n",
    "print(\"Response code from API call = %d\" % rc)\n",
    "print(\"JSON response from API call =  %s\" % jsonresp)\n",
    "# If an object was found and determined to be a certain class, it is in the section called 'classified'\n",
    "# If there is no 'classified' section in the JSON, then it indicates no objects were found\n",
    "if 'classified' in jsonresp:\n",
    "    print(\"\\nGot back %d objects\" % len(jsonresp['classified']))\n",
    "print(json.dumps(jsonresp, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Get object detection results for sampled frames</h1>\n",
    "\n",
    "Since we've stored all video frames on disk (for easy reference), we can iterate over those files and make queries as appropriate to PowerAI Vision's API. We'll store the results in a tracking_results dictionary, organized by file name. Since we are tracking objects from frame to frame, we can use sampling to decide how often to check for new objects.\n",
    "\n",
    "We're also caching the results so that you can change later code and run the notebook over without running the same inference over again.  Thisis a HUGE time saver, especially for larger videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serialize requests, storing them in a \"tracking_results\" dict\n",
    "# This process is slow if the video is long and you are running on your local machine.  It's better to run this\n",
    "# part on the server and copy the files back to your local machine.\n",
    "\n",
    "try:\n",
    "    with open('frames/frame-data-newmodel.json') as existing_results:\n",
    "        tracking_results = json.load(existing_results)\n",
    "except Exception:\n",
    "    # Any fail to read existing results means we start over\n",
    "    tracking_results = {}\n",
    "\n",
    "print(\"Sampling every %s frames\" % SAMPLING)\n",
    "i = 0\n",
    "cache_used = 0\n",
    "sampled = 0\n",
    "for filename in sorted(glob.glob('frames/frame_*.jpg')):\n",
    "    i += 1\n",
    "    # If running the notebook on Windows, \\ has to be replaced with /\n",
    "    # or it will throw and error\n",
    "    if WINDOWS == \"Y\":\n",
    "        filename = filename.replace('\\\\', '/')\n",
    "\n",
    "    if not i % SAMPLING == 0:  # Sample every Nth\n",
    "        continue\n",
    "\n",
    "    existing_result = tracking_results.get(filename)\n",
    "    if existing_result and existing_result['result'] == 'success':\n",
    "        cache_used += 1\n",
    "    else:\n",
    "        rc, results = detect_objects(filename)\n",
    "        if rc != 200 or results['result'] != 'success':\n",
    "            print(\"ERROR rc=%d for %s\" % (rc, filename))\n",
    "            print(\"\\n\")\n",
    "            print(\"ERROR result=%s\" % results)\n",
    "            print(\"\\n\")\n",
    "        else:\n",
    "            sampled += 1\n",
    "            # Save frequently to cache partial results\n",
    "            tracking_results[filename] = results\n",
    "            with open('frames/frame-data-newmodel.json', 'w') as fp:\n",
    "                json.dump(tracking_results, fp)\n",
    "\n",
    "    print(\"Processed file {num} of {total_frames} (used cache {cache_used} times)\".format(num=i, total_frames=total_frames, cache_used=cache_used), end=\"\\r\")\n",
    "\n",
    "# Finally, write all our results\n",
    "with open('frames/frame-data-newmodel.json', 'w') as fp:\n",
    "    json.dump(tracking_results, fp)\n",
    "\n",
    "print(\"\\nDone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Inference, tracking, and annotation</h1>\n",
    "\n",
    "Loop through the saved frames and draw the bounding box.  Also, write out the word that is being spelled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('frames/frame-data-newmodel.json') as existing_results:\n",
    "    tracking_results = json.load(existing_results)\n",
    "\n",
    "# previousletters is used to keep track of labels that were above CONFIDENCE    \n",
    "previousletters = [' ']\n",
    "# currentword is the current word that is being spelled\n",
    "currentword = ''\n",
    "# Used to determine if a space is needed\n",
    "spacecounter = 0\n",
    "# Used to decide how many images to process before showing progress\n",
    "counter = 0\n",
    "\n",
    "for filename in sorted(glob.glob('frames/frame_*.jpg')):\n",
    "    counter += 1\n",
    "    # Only show progress every 20 images\n",
    "    if counter % 20 == 0:\n",
    "        print(\"{counter} of {total_frames} files processed\".format(counter=counter, total_frames=total_frames), end=\"\\r\")\n",
    "    # If running the notebook on Windows, \\ has to be replaced with /\n",
    "    # or it will throw and error\n",
    "    if WINDOWS == \"Y\":\n",
    "        filename = filename.replace('\\\\', '/')\n",
    "    img = cv2.imread(filename)\n",
    "    # if filename in tracking_results:\n",
    "    if filename in tracking_results and 'classified' in tracking_results[filename]:\n",
    "        jsonresp = tracking_results[filename]\n",
    "        for obj in jsonresp['classified']:\n",
    "\n",
    "            color = (80, 220, 60)\n",
    "            thickness = 4\n",
    "            textsize = 5\n",
    "            xmin = obj['xmin']\n",
    "            ymin = obj['ymin']\n",
    "            xmax = obj['xmax']\n",
    "            ymax = obj['ymax']\n",
    "            label = obj['label']\n",
    "            if obj['confidence'] > CONFIDENCE:\n",
    "                # Only look at letters that have occurred more than 5 times in the last 9 times.\n",
    "                # I have experimented with other values and this seems to work well at present\n",
    "                # This keeps it from showing spurious, incorrect letters\n",
    "                if(previousletters.count(label) > 5):\n",
    "                    # See if the current label matches the last letter of the current word.\n",
    "                    # If it does, do not add it to currentword\n",
    "                    if(currentword[-1:] != label):\n",
    "                        currentword = currentword + label\n",
    "                # Add the current label to previousletters\n",
    "                previousletters.append(label)\n",
    "                # If previousletters has 9 or more items, then remove the first one.  This keeps the\n",
    "                # list from getting too long. \n",
    "                if (len(previousletters) >= 9):\n",
    "                    previousletters.pop(0)\n",
    "                \n",
    "                # Draw the bounding box on the image\n",
    "                cv2.rectangle(img, (xmin,ymin), (xmax, ymax), color, thickness)\n",
    "                # Since there was a bounding box drawn, spacecounter can be reset\n",
    "                spacecounter = 0\n",
    "            else:\n",
    "                # No bounding box was drawn, so increment spacecounter\n",
    "                spacecounter += 1\n",
    "    else:\n",
    "        spacecounter += 1\n",
    "    # If spacecounter hits a value or 10 or more and the last letter of current word is not a space,\n",
    "    # this means a space should be added.\n",
    "    if (spacecounter >= 10) and (currentword[-1:] != ' '):\n",
    "        currentword = currentword + ' '\n",
    "    # Write the text to the top left corner of the image\n",
    "    cv2.putText(img, 'Word: ' + currentword, (30, 60), cv2.FONT_HERSHEY_SIMPLEX, 1.5, WHITE, 4, cv2.LINE_AA)\n",
    "    \n",
    "    # Write the image to a file\n",
    "    cv2.imwrite(\"output/output-\" + filename.split('/')[1], img)\n",
    "\n",
    "print(\"\\nDone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Play the annotated frames in the notebook</h1>\n",
    "\n",
    "This code will play the annotated frames in a loop to demonstrate the new video. This can be a little choppy if tracking is not employed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in sorted(glob.glob(os.path.join(os.path.abspath(OUTPUT_DIR),\n",
    "                                              'output-frame_*.jpg'))):\n",
    "    frame = cv2.imread(filename)\n",
    "    clear_output(wait=True)\n",
    "    rows, columns, _channels = frame.shape\n",
    "    frame = cv2.resize(frame, (int(columns/2), int(rows/2)))  # shrink it\n",
    "    _ret, jpg = cv2.imencode('.jpg', frame)\n",
    "    display(Image(data=jpg))\n",
    "\n",
    "print(\"\\nDone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Create a video from the annotated frames</h1>\n",
    "\n",
    "This command requires ffmpeg. It will combine the annotated frames to build an MP4 video which you can play at full speed (the notebook playback above was most likely slow).\n",
    "\n",
    "Uncomment the command to try running it from this notebook, or copy the output files to a system with ffmpeg and run the command there.\n",
    "\n",
    "NOTE: The command below requires libx264 for encoding video stream into the H.264/MPEG-4 AVC compression format. Please check that ffmpeg was configured and built with --enable-libx264 (ffmpeg 2>&1 | grep libx264). If not, just remove the -vcodec libx264 option from the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!ffmpeg -y -r 60 -f image2 -i output/output-frame_%05d.jpg -vcodec libx264 -crf 25  -pix_fmt yuvj420p annotated_video.mp4\n",
    "!ffmpeg -y -r 60 -f image2 -i output/output-frame_%05d.jpg -crf 25  -pix_fmt yuvj420p annotated_video.mp4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
